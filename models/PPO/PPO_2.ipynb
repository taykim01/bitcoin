{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BitcoinTradingEnv(gym.Env):\n",
    "    def __init__(self, initial_balance=1000000, transaction_cost=0.0005, window_size=96):\n",
    "        super(BitcoinTradingEnv, self).__init__()\n",
    "        \n",
    "        self.initial_balance = initial_balance\n",
    "        self.transaction_cost = transaction_cost\n",
    "        self.window_size = window_size\n",
    "        \n",
    "        self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(self.window_size, 7), dtype=np.float32)\n",
    "        \n",
    "        self.action_space = gym.spaces.Box(low=np.array([-100]), high=np.array([100]), dtype=np.float32)\n",
    "\n",
    "    def reset(self, data):\n",
    "        self.data = data\n",
    "        self.balance = self.initial_balance\n",
    "        self.bitcoin_held = 0\n",
    "        self.current_step = self.window_size\n",
    "        self.done = False\n",
    "        self.total_asset_value = self.balance\n",
    "        self.trade_direction = 0\n",
    "        \n",
    "        return self._next_observation()\n",
    "\n",
    "    def _next_observation(self):\n",
    "        frame = self.data.iloc[self.current_step - self.window_size:self.current_step]\n",
    "        \n",
    "        # Pad the frame if it's smaller than window_size\n",
    "        if len(frame) < self.window_size:\n",
    "            padding = self.window_size - len(frame)\n",
    "            frame = frame.reindex(range(-padding, len(frame)))\n",
    "            frame.iloc[:padding] = frame.iloc[padding]\n",
    "        \n",
    "        obs = np.column_stack((\n",
    "            frame['open'].values,\n",
    "            frame['high'].values,\n",
    "            frame['low'].values,\n",
    "            frame['close'].values,\n",
    "            frame['volume'].values,\n",
    "            frame['value'].values,\n",
    "            np.full(self.window_size, self.balance)\n",
    "        )).astype(np.float32)\n",
    "        \n",
    "        return obs\n",
    "\n",
    "    def step(self, action):\n",
    "        current_price = self.data.iloc[self.current_step]['close']\n",
    "        \n",
    "        if action > 0:\n",
    "            bitcoin_bought = (self.balance / current_price) * action\n",
    "            self.bitcoin_held += bitcoin_bought\n",
    "            self.balance -= self.balance * action * (1 + self.transaction_cost)\n",
    "            \n",
    "        elif action < 0:\n",
    "            bitcoin_sold = self.bitcoin_held * (-action)\n",
    "            self.balance += current_price * bitcoin_sold * (1 - self.transaction_cost)\n",
    "            self.bitcoin_held -= bitcoin_sold\n",
    "\n",
    "        self.balance = float(self.balance)\n",
    "        self.current_step += 1\n",
    "        if self.current_step >= len(self.data) - 1:\n",
    "            self.done = True\n",
    "\n",
    "        self.total_asset_value = self.balance + self.bitcoin_held * current_price\n",
    "        reward = float(self.total_asset_value - self.initial_balance)\n",
    "        next_state = self._next_observation()\n",
    "        self.trade_direction += action\n",
    "\n",
    "        return next_state, reward, self.done, {}\n",
    "\n",
    "    def render(self):\n",
    "        profit = self.total_asset_value - self.initial_balance\n",
    "        print(f\"Total Asset Value: {self.total_asset_value:.2f}, Profit: {profit:.2f}, Trade Direction: {self.trade_direction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, action_dim)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        mean = torch.tanh(self.fc3(x))\n",
    "        return Normal(mean, torch.ones_like(mean) * 0.1)\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        value = self.fc3(x)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOAgent:\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=64, lr=1e-4, gamma=0.99, clip_epsilon=0.2):\n",
    "        self.actor = Actor(state_dim, action_dim, hidden_dim)\n",
    "        self.critic = Critic(state_dim, hidden_dim)\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.clip_epsilon = clip_epsilon\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.FloatTensor(state).reshape(1, -1)  # Reshape to (1, state_dim)\n",
    "        dist = self.actor(state)\n",
    "        action = dist.sample()\n",
    "        action_clipped = torch.clamp(action, -1, 1)\n",
    "        log_prob = dist.log_prob(action)\n",
    "        return action_clipped.detach().numpy()[0], log_prob.detach()\n",
    "\n",
    "    def compute_advantage(self, rewards, values, next_values, dones):\n",
    "        advantages = []\n",
    "        returns = []\n",
    "        advantage = 0\n",
    "        for step in reversed(range(len(rewards))):\n",
    "            td_error = rewards[step] + self.gamma * next_values[step] * (1 - dones[step]) - values[step]\n",
    "            advantage = td_error + self.gamma * advantage * (1 - dones[step])\n",
    "            advantages.insert(0, advantage)\n",
    "            returns.insert(0, advantage + values[step])\n",
    "        return advantages, returns\n",
    "\n",
    "    def update(self, states, actions, log_probs, returns, advantages):\n",
    "        advantages = torch.FloatTensor(advantages)\n",
    "        returns = torch.FloatTensor(returns)\n",
    "        states = torch.FloatTensor(np.array(states))  # Convert list to numpy array first\n",
    "        actions = torch.FloatTensor(actions)\n",
    "        log_probs_old = torch.FloatTensor(log_probs)\n",
    "\n",
    "        # Reshape states to (batch_size, window_size * 7)\n",
    "        states = states.reshape(states.shape[0], -1)\n",
    "\n",
    "        for _ in range(10):\n",
    "            dist = self.actor(states)\n",
    "            log_probs_new = dist.log_prob(actions)\n",
    "            ratio = torch.exp(log_probs_new - log_probs_old)\n",
    "            surr1 = ratio * advantages\n",
    "            surr2 = torch.clamp(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * advantages\n",
    "            actor_loss = -torch.min(surr1, surr2).mean()\n",
    "            self.actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            self.actor_optimizer.step()\n",
    "\n",
    "        values = self.critic(states).squeeze()\n",
    "        critic_loss = (returns - values).pow(2).mean()\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/taykim01/Desktop/bitcoin/venv/lib/python3.12/site-packages/gym/spaces/box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('../../data/dataset/KRW-BTC_recent.csv')\n",
    "chunk_size = 10000\n",
    "data_chunks = [data.iloc[i:i+chunk_size] for i in range(0, len(data), chunk_size)]\n",
    "\n",
    "env = BitcoinTradingEnv(window_size=96)\n",
    "state_dim = env.observation_space.shape\n",
    "action_dim = env.action_space.shape[0]\n",
    "\n",
    "agent = PPOAgent(state_dim[0] * state_dim[1], action_dim, hidden_dim=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rewards(all_rewards):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(all_rewards, label='Rewards per Episode')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Total Reward')\n",
    "    plt.title('Total Rewards Over Episodes')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:   0%|          | 0/1000 [00:00<?, ?it/s]/var/folders/xp/57hcj_316ln6ltkscjnhnb240000gn/T/ipykernel_2171/3565914114.py:58: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  self.balance = float(self.balance)\n",
      "/var/folders/xp/57hcj_316ln6ltkscjnhnb240000gn/T/ipykernel_2171/3565914114.py:64: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  reward = float(self.total_asset_value - self.initial_balance)\n",
      "Training...:   0%|          | 0/1000 [00:05<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected parameter loc (Tensor of shape (1, 1)) of distribution Normal(loc: tensor([[nan]], grad_fn=<TanhBackward0>), scale: tensor([[0.1000]])) to satisfy the constraint Real(), but found invalid values:\ntensor([[nan]], grad_fn=<TanhBackward0>)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[220], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m dones \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(update_timesteps):\n\u001b[0;32m---> 17\u001b[0m     action, log_prob \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     obs_flattened \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(obs)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     20\u001b[0m     value \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mcritic(obs_flattened\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m))\u001b[38;5;241m.\u001b[39mitem()\n",
      "Cell \u001b[0;32mIn[217], line 12\u001b[0m, in \u001b[0;36mPPOAgent.select_action\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect_action\u001b[39m(\u001b[38;5;28mself\u001b[39m, state):\n\u001b[1;32m     11\u001b[0m     state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(state)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Reshape to (1, state_dim)\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     dist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     action \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39msample()\n\u001b[1;32m     14\u001b[0m     action_clipped \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mclamp(action, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/bitcoin/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/bitcoin/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[216], line 12\u001b[0m, in \u001b[0;36mActor.forward\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     10\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x))\n\u001b[1;32m     11\u001b[0m mean \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtanh(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc3(x))\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mNormal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/bitcoin/venv/lib/python3.12/site-packages/torch/distributions/normal.py:57\u001b[0m, in \u001b[0;36mNormal.__init__\u001b[0;34m(self, loc, scale, validate_args)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     56\u001b[0m     batch_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloc\u001b[38;5;241m.\u001b[39msize()\n\u001b[0;32m---> 57\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/bitcoin/venv/lib/python3.12/site-packages/torch/distributions/distribution.py:70\u001b[0m, in \u001b[0;36mDistribution.__init__\u001b[0;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[1;32m     68\u001b[0m         valid \u001b[38;5;241m=\u001b[39m constraint\u001b[38;5;241m.\u001b[39mcheck(value)\n\u001b[1;32m     69\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m valid\u001b[38;5;241m.\u001b[39mall():\n\u001b[0;32m---> 70\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     71\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected parameter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     72\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(value\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     73\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof distribution \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     74\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto satisfy the constraint \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(constraint)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     75\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut found invalid values:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     76\u001b[0m             )\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[0;31mValueError\u001b[0m: Expected parameter loc (Tensor of shape (1, 1)) of distribution Normal(loc: tensor([[nan]], grad_fn=<TanhBackward0>), scale: tensor([[0.1000]])) to satisfy the constraint Real(), but found invalid values:\ntensor([[nan]], grad_fn=<TanhBackward0>)"
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "update_timesteps=2000\n",
    "all_rewards = []\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    for index, chunk in enumerate(data_chunks):\n",
    "        obs = env.reset(chunk)\n",
    "        episode_reward = 0\n",
    "        log_probs = []\n",
    "        values = []\n",
    "        rewards = []\n",
    "        states = []\n",
    "        actions = []\n",
    "        dones = []\n",
    "\n",
    "        for t in range(update_timesteps):\n",
    "            action, log_prob = agent.select_action(obs)\n",
    "            value = agent.critic(torch.FloatTensor(obs).unsqueeze(0)).item()\n",
    "\n",
    "            next_obs, reward, done, _ = env.step(action)\n",
    "            episode_reward += reward\n",
    "\n",
    "            log_probs.append(log_prob)\n",
    "            values.append(value)\n",
    "            rewards.append(reward)\n",
    "            states.append(obs)\n",
    "            actions.append(action)\n",
    "            dones.append(done)\n",
    "\n",
    "            obs = next_obs\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        next_value = agent.critic(torch.FloatTensor(obs).unsqueeze(0)).item()\n",
    "        values.append(next_value)\n",
    "        advantages, returns = agent.compute_advantage(rewards, values[:-1], values[1:], dones)\n",
    "\n",
    "        agent.update(states, actions, log_probs, returns, advantages)\n",
    "        all_rewards.append(episode_reward)\n",
    "        \n",
    "    if not epoch == 0 and epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}/{epochs}, Reward: {np.average(all_rewards[-100:])}')\n",
    "        \n",
    "plot_rewards(all_rewards)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
